{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "#import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spect = []\n",
    "spect_id = []\n",
    "for j in ['Parkinsons','NonParkinsons']:\n",
    "    file = []\n",
    "    for root, dirs, files in os.walk('spectrograms/'+j):\n",
    "        file += glob.glob(os.path.join(root,'*.png'))\n",
    "    for i in file:\n",
    "        if j == 'NonParkinsons':\n",
    "            x = plt.imread(i) \n",
    "        if j == 'Parkinsons':\n",
    "            x = plt.imread(i)\n",
    "        spect.append(x)\n",
    "        spect_id.append(j)\n",
    "    \n",
    "x_train = spect\n",
    "y_train = spect_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2105"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2105\n"
     ]
    }
   ],
   "source": [
    "#create subset of data for test and train\n",
    "import random\n",
    "print(len(x_train))\n",
    "test = 140*.2\n",
    "test_indices = random.sample(range(len(x_train)), 630)\n",
    "train_indices = []\n",
    "for i in range(len(x_train)):\n",
    "    if i not in test_indices:\n",
    "        train_indices.append(i)\n",
    "\n",
    "x_new_train = []\n",
    "y_new_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in test_indices:\n",
    "    x_test.append(x_train[i])\n",
    "    y_test.append(y_train[i])\n",
    "\n",
    "\n",
    "for i in train_indices:\n",
    "    x_new_train.append(x_train[i])\n",
    "    y_new_train.append(y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_temp_train = y_new_train\n",
    "encoder.fit(y_temp_train)\n",
    "encoded_Y = encoder.transform(y_temp_train)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "y_temp2_train = y_test\n",
    "encoder.fit(y_temp2_train)\n",
    "encoded_Y = encoder.transform(y_temp2_train)\n",
    "dummy2_y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'channels_last'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras import optimizers\n",
    "from keras.layers.advanced_activations import ELU, PReLU, LeakyReLU\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/bryanmckinney/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/bryanmckinney/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 288, 432, 8)       296       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 288, 432, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 286, 430, 8)       584       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 286, 430, 8)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 143, 215, 8)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 143, 215, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 143, 215, 16)      1168      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 143, 215, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 141, 213, 16)      2320      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 141, 213, 16)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 70, 106, 16)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 118720)            0         \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 118720)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 237442    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 241,810\n",
      "Trainable params: 241,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(8, (3, 3), padding='same',\n",
    "                 input_shape=(288, 432, 4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(8, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(16, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(10))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/bryanmckinney/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1475 samples, validate on 630 samples\n",
      "Epoch 1/500\n",
      "1475/1475 [==============================] - 84s 57ms/step - loss: 0.7631 - acc: 0.4922 - val_loss: 0.7142 - val_acc: 0.4825\n",
      "Epoch 2/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.7079 - acc: 0.5214 - val_loss: 0.7329 - val_acc: 0.4825\n",
      "Epoch 3/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.6825 - acc: 0.5783 - val_loss: 0.6856 - val_acc: 0.5175\n",
      "Epoch 4/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 0.6481 - acc: 0.6210 - val_loss: 0.6548 - val_acc: 0.5841\n",
      "Epoch 5/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.5846 - acc: 0.7003 - val_loss: 0.8563 - val_acc: 0.5175\n",
      "Epoch 6/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.5618 - acc: 0.7227 - val_loss: 0.5901 - val_acc: 0.6952\n",
      "Epoch 7/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.5212 - acc: 0.7369 - val_loss: 0.5822 - val_acc: 0.7079\n",
      "Epoch 8/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.4950 - acc: 0.7634 - val_loss: 0.5730 - val_acc: 0.7127\n",
      "Epoch 9/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.4675 - acc: 0.7715 - val_loss: 0.5967 - val_acc: 0.6921\n",
      "Epoch 10/500\n",
      "1475/1475 [==============================] - 84s 57ms/step - loss: 0.4476 - acc: 0.8027 - val_loss: 0.5555 - val_acc: 0.7317\n",
      "Epoch 11/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.4190 - acc: 0.8108 - val_loss: 0.5379 - val_acc: 0.7175\n",
      "Epoch 12/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.3869 - acc: 0.8312 - val_loss: 0.5379 - val_acc: 0.7429\n",
      "Epoch 13/500\n",
      "1475/1475 [==============================] - 75s 51ms/step - loss: 0.3616 - acc: 0.8420 - val_loss: 0.5910 - val_acc: 0.6762\n",
      "Epoch 14/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.3331 - acc: 0.8603 - val_loss: 0.5301 - val_acc: 0.7286\n",
      "Epoch 15/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.3125 - acc: 0.8664 - val_loss: 0.7506 - val_acc: 0.6254\n",
      "Epoch 16/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.3018 - acc: 0.8753 - val_loss: 0.5835 - val_acc: 0.7333\n",
      "Epoch 17/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.2698 - acc: 0.8929 - val_loss: 0.5481 - val_acc: 0.7270\n",
      "Epoch 18/500\n",
      "1475/1475 [==============================] - 80s 54ms/step - loss: 0.2383 - acc: 0.9085 - val_loss: 0.5962 - val_acc: 0.7032\n",
      "Epoch 19/500\n",
      "1475/1475 [==============================] - 80s 54ms/step - loss: 0.2271 - acc: 0.9180 - val_loss: 0.5378 - val_acc: 0.7508\n",
      "Epoch 20/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.2078 - acc: 0.9159 - val_loss: 0.5692 - val_acc: 0.7651\n",
      "Epoch 21/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 0.1925 - acc: 0.9275 - val_loss: 0.5475 - val_acc: 0.7508\n",
      "Epoch 22/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.1808 - acc: 0.9302 - val_loss: 0.6701 - val_acc: 0.7000\n",
      "Epoch 23/500\n",
      "1475/1475 [==============================] - 77s 53ms/step - loss: 0.1602 - acc: 0.9424 - val_loss: 0.5807 - val_acc: 0.7429\n",
      "Epoch 24/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.1508 - acc: 0.9424 - val_loss: 0.6342 - val_acc: 0.7540\n",
      "Epoch 25/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.1289 - acc: 0.9593 - val_loss: 1.1527 - val_acc: 0.6190\n",
      "Epoch 26/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 0.1242 - acc: 0.9614 - val_loss: 0.6476 - val_acc: 0.7317\n",
      "Epoch 27/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0959 - acc: 0.9688 - val_loss: 0.6675 - val_acc: 0.7492\n",
      "Epoch 28/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0936 - acc: 0.9756 - val_loss: 0.6180 - val_acc: 0.7619\n",
      "Epoch 29/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0831 - acc: 0.9776 - val_loss: 0.6399 - val_acc: 0.7460\n",
      "Epoch 30/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0750 - acc: 0.9763 - val_loss: 0.6412 - val_acc: 0.7508\n",
      "Epoch 31/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0623 - acc: 0.9797 - val_loss: 0.6729 - val_acc: 0.7460\n",
      "Epoch 32/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0574 - acc: 0.9817 - val_loss: 0.6922 - val_acc: 0.7444\n",
      "Epoch 33/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0862 - acc: 0.9661 - val_loss: 0.7084 - val_acc: 0.7333\n",
      "Epoch 34/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0419 - acc: 0.9871 - val_loss: 0.7936 - val_acc: 0.7381\n",
      "Epoch 35/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0430 - acc: 0.9864 - val_loss: 0.7313 - val_acc: 0.7698\n",
      "Epoch 36/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0406 - acc: 0.9892 - val_loss: 0.7545 - val_acc: 0.7571\n",
      "Epoch 37/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0488 - acc: 0.9797 - val_loss: 0.8266 - val_acc: 0.7381\n",
      "Epoch 38/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 0.0332 - acc: 0.9919 - val_loss: 0.7885 - val_acc: 0.7571\n",
      "Epoch 39/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0533 - acc: 0.9810 - val_loss: 0.8004 - val_acc: 0.7524\n",
      "Epoch 40/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0085 - acc: 1.0000 - val_loss: 1.2507 - val_acc: 0.6778\n",
      "Epoch 41/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0273 - acc: 0.9912 - val_loss: 0.8484 - val_acc: 0.7619\n",
      "Epoch 42/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0385 - acc: 0.9878 - val_loss: 0.8591 - val_acc: 0.7556\n",
      "Epoch 43/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 0.0254 - acc: 0.9892 - val_loss: 0.8570 - val_acc: 0.7587\n",
      "Epoch 44/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0322 - acc: 0.9898 - val_loss: 0.9214 - val_acc: 0.7444\n",
      "Epoch 45/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0315 - acc: 0.9919 - val_loss: 0.8909 - val_acc: 0.7492\n",
      "Epoch 46/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0182 - acc: 0.9939 - val_loss: 1.3019 - val_acc: 0.7095\n",
      "Epoch 47/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.9736 - val_acc: 0.7571\n",
      "Epoch 48/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0236 - acc: 0.9919 - val_loss: 0.9280 - val_acc: 0.7619\n",
      "Epoch 49/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0181 - acc: 0.9946 - val_loss: 1.2055 - val_acc: 0.7222\n",
      "Epoch 50/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0406 - acc: 0.9885 - val_loss: 0.9818 - val_acc: 0.7587\n",
      "Epoch 51/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 1.1025 - val_acc: 0.7397\n",
      "Epoch 52/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0262 - acc: 0.9946 - val_loss: 1.0399 - val_acc: 0.7603\n",
      "Epoch 53/500\n",
      "1475/1475 [==============================] - 80s 54ms/step - loss: 0.0324 - acc: 0.9919 - val_loss: 1.0810 - val_acc: 0.7429\n",
      "Epoch 54/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 0.0048 - acc: 0.9980 - val_loss: 1.2134 - val_acc: 0.7365\n",
      "Epoch 55/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 1.1052 - val_acc: 0.7492\n",
      "Epoch 56/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0317 - acc: 0.9898 - val_loss: 1.0774 - val_acc: 0.7492\n",
      "Epoch 57/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0274 - acc: 0.9912 - val_loss: 1.0719 - val_acc: 0.7444\n",
      "Epoch 58/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1475/1475 [==============================] - 78s 53ms/step - loss: 6.8627e-04 - acc: 1.0000 - val_loss: 1.0909 - val_acc: 0.7619\n",
      "Epoch 59/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0362 - acc: 0.9871 - val_loss: 1.0741 - val_acc: 0.7635\n",
      "Epoch 60/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 5.7757e-04 - acc: 1.0000 - val_loss: 1.1378 - val_acc: 0.7540\n",
      "Epoch 61/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0189 - acc: 0.9925 - val_loss: 1.0972 - val_acc: 0.7571\n",
      "Epoch 62/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0272 - acc: 0.9919 - val_loss: 1.2557 - val_acc: 0.7413\n",
      "Epoch 63/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 6.5314e-04 - acc: 1.0000 - val_loss: 1.1169 - val_acc: 0.7603\n",
      "Epoch 64/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0242 - acc: 0.9946 - val_loss: 1.1557 - val_acc: 0.7603\n",
      "Epoch 65/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 6.6541e-04 - acc: 1.0000 - val_loss: 1.1992 - val_acc: 0.7683\n",
      "Epoch 66/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0209 - acc: 0.9939 - val_loss: 1.1453 - val_acc: 0.7619\n",
      "Epoch 67/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 1.1673 - val_acc: 0.7762\n",
      "Epoch 68/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0132 - acc: 0.9966 - val_loss: 1.1389 - val_acc: 0.7667\n",
      "Epoch 69/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 2.8807e-04 - acc: 1.0000 - val_loss: 1.1834 - val_acc: 0.7667\n",
      "Epoch 70/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0225 - acc: 0.9919 - val_loss: 1.2112 - val_acc: 0.7587\n",
      "Epoch 71/500\n",
      "1475/1475 [==============================] - 83s 56ms/step - loss: 0.0465 - acc: 0.9905 - val_loss: 1.4438 - val_acc: 0.7317\n",
      "Epoch 72/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0017 - acc: 0.9993 - val_loss: 1.2028 - val_acc: 0.7587\n",
      "Epoch 73/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 2.0907e-04 - acc: 1.0000 - val_loss: 1.2324 - val_acc: 0.7635\n",
      "Epoch 74/500\n",
      "1475/1475 [==============================] - 79s 54ms/step - loss: 0.0198 - acc: 0.9939 - val_loss: 1.2460 - val_acc: 0.7651\n",
      "Epoch 75/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 1.6570e-04 - acc: 1.0000 - val_loss: 1.2650 - val_acc: 0.7619\n",
      "Epoch 76/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 0.0132 - acc: 0.9966 - val_loss: 1.3194 - val_acc: 0.7714\n",
      "Epoch 77/500\n",
      "1475/1475 [==============================] - 79s 54ms/step - loss: 1.3248e-04 - acc: 1.0000 - val_loss: 1.3318 - val_acc: 0.7746\n",
      "Epoch 78/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 0.0397 - acc: 0.9905 - val_loss: 1.5270 - val_acc: 0.7365\n",
      "Epoch 79/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.5750e-04 - acc: 1.0000 - val_loss: 1.3499 - val_acc: 0.7683\n",
      "Epoch 80/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 7.7583e-05 - acc: 1.0000 - val_loss: 1.3810 - val_acc: 0.7651\n",
      "Epoch 81/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0453 - acc: 0.9925 - val_loss: 1.4155 - val_acc: 0.7413\n",
      "Epoch 82/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.2361e-04 - acc: 1.0000 - val_loss: 1.3455 - val_acc: 0.7587\n",
      "Epoch 83/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0332 - acc: 0.9946 - val_loss: 1.3392 - val_acc: 0.7667\n",
      "Epoch 84/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1586e-04 - acc: 1.0000 - val_loss: 1.4339 - val_acc: 0.7492\n",
      "Epoch 85/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0167 - acc: 0.9946 - val_loss: 1.3581 - val_acc: 0.7730\n",
      "Epoch 86/500\n",
      "1475/1475 [==============================] - 77s 53ms/step - loss: 2.1751e-04 - acc: 1.0000 - val_loss: 1.4196 - val_acc: 0.7540\n",
      "Epoch 87/500\n",
      "1475/1475 [==============================] - 80s 55ms/step - loss: 8.1294e-05 - acc: 1.0000 - val_loss: 1.6279 - val_acc: 0.7365\n",
      "Epoch 88/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0192 - acc: 0.9925 - val_loss: 1.4924 - val_acc: 0.7429\n",
      "Epoch 89/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 4.9688e-05 - acc: 1.0000 - val_loss: 1.4581 - val_acc: 0.7571\n",
      "Epoch 90/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0155 - acc: 0.9946 - val_loss: 1.4170 - val_acc: 0.7651\n",
      "Epoch 91/500\n",
      "1475/1475 [==============================] - 80s 54ms/step - loss: 7.6705e-05 - acc: 1.0000 - val_loss: 1.4588 - val_acc: 0.7508\n",
      "Epoch 92/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 3.0285e-05 - acc: 1.0000 - val_loss: 1.4831 - val_acc: 0.7571\n",
      "Epoch 93/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0261 - acc: 0.9939 - val_loss: 1.6139 - val_acc: 0.7571\n",
      "Epoch 94/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 4.5878e-05 - acc: 1.0000 - val_loss: 1.4907 - val_acc: 0.7556\n",
      "Epoch 95/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0236 - acc: 0.9932 - val_loss: 1.4456 - val_acc: 0.7683\n",
      "Epoch 96/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 2.3026e-04 - acc: 1.0000 - val_loss: 1.4428 - val_acc: 0.7603\n",
      "Epoch 97/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 2.6355e-05 - acc: 1.0000 - val_loss: 1.6637 - val_acc: 0.7429\n",
      "Epoch 98/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0201 - acc: 0.9953 - val_loss: 1.6213 - val_acc: 0.7444\n",
      "Epoch 99/500\n",
      "1475/1475 [==============================] - 81s 55ms/step - loss: 3.7915e-05 - acc: 1.0000 - val_loss: 1.5682 - val_acc: 0.7571\n",
      "Epoch 100/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0087 - acc: 0.9959 - val_loss: 1.7056 - val_acc: 0.7476\n",
      "Epoch 101/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 4.6990e-05 - acc: 1.0000 - val_loss: 1.5472 - val_acc: 0.7508\n",
      "Epoch 102/500\n",
      "1475/1475 [==============================] - 77s 53ms/step - loss: 2.3392e-05 - acc: 1.0000 - val_loss: 1.9973 - val_acc: 0.7222\n",
      "Epoch 103/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0364 - acc: 0.9939 - val_loss: 1.5315 - val_acc: 0.7698\n",
      "Epoch 104/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 2.5780e-05 - acc: 1.0000 - val_loss: 1.5978 - val_acc: 0.7540\n",
      "Epoch 105/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.7499e-05 - acc: 1.0000 - val_loss: 1.6722 - val_acc: 0.7508\n",
      "Epoch 106/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0126 - acc: 0.9966 - val_loss: 1.6805 - val_acc: 0.7476\n",
      "Epoch 107/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.6551e-05 - acc: 1.0000 - val_loss: 1.5821 - val_acc: 0.7460\n",
      "Epoch 108/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0112 - acc: 0.9966 - val_loss: 1.5760 - val_acc: 0.7524\n",
      "Epoch 109/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 4.0948e-05 - acc: 1.0000 - val_loss: 1.5758 - val_acc: 0.7508\n",
      "Epoch 110/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 2.3029e-05 - acc: 1.0000 - val_loss: 1.5874 - val_acc: 0.7508\n",
      "Epoch 111/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0224 - acc: 0.9953 - val_loss: 1.6216 - val_acc: 0.7524\n",
      "Epoch 112/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.0955e-05 - acc: 1.0000 - val_loss: 1.6360 - val_acc: 0.7524\n",
      "Epoch 113/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 7.9772e-06 - acc: 1.0000 - val_loss: 1.6553 - val_acc: 0.7556\n",
      "Epoch 114/500\n",
      "1475/1475 [==============================] - 80s 54ms/step - loss: 0.0209 - acc: 0.9932 - val_loss: 1.6023 - val_acc: 0.7651\n",
      "Epoch 115/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 4.1940e-05 - acc: 1.0000 - val_loss: 1.6403 - val_acc: 0.7619\n",
      "Epoch 116/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1475/1475 [==============================] - 77s 52ms/step - loss: 2.8984e-05 - acc: 1.0000 - val_loss: 2.8372 - val_acc: 0.6683\n",
      "Epoch 117/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0048 - acc: 0.9980 - val_loss: 1.6495 - val_acc: 0.7460\n",
      "Epoch 118/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 1.5250e-05 - acc: 1.0000 - val_loss: 1.6241 - val_acc: 0.7635\n",
      "Epoch 119/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0130 - acc: 0.9966 - val_loss: 1.6206 - val_acc: 0.7683\n",
      "Epoch 120/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 2.5912e-05 - acc: 1.0000 - val_loss: 1.6789 - val_acc: 0.7556\n",
      "Epoch 121/500\n",
      "1475/1475 [==============================] - 81s 55ms/step - loss: 9.2351e-06 - acc: 1.0000 - val_loss: 1.6444 - val_acc: 0.7683\n",
      "Epoch 122/500\n",
      "1475/1475 [==============================] - 82s 56ms/step - loss: 0.0078 - acc: 0.9973 - val_loss: 1.7644 - val_acc: 0.7476\n",
      "Epoch 123/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 8.0393e-06 - acc: 1.0000 - val_loss: 1.6813 - val_acc: 0.7540\n",
      "Epoch 124/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0078 - acc: 0.9966 - val_loss: 1.9045 - val_acc: 0.7381\n",
      "Epoch 125/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 4.2487e-05 - acc: 1.0000 - val_loss: 1.7529 - val_acc: 0.7603\n",
      "Epoch 126/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 8.5616e-06 - acc: 1.0000 - val_loss: 1.7577 - val_acc: 0.7476\n",
      "Epoch 127/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0197 - acc: 0.9953 - val_loss: 1.7017 - val_acc: 0.7635\n",
      "Epoch 128/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.6307e-05 - acc: 1.0000 - val_loss: 1.7187 - val_acc: 0.7667\n",
      "Epoch 129/500\n",
      "1475/1475 [==============================] - 76s 52ms/step - loss: 7.4465e-06 - acc: 1.0000 - val_loss: 1.7306 - val_acc: 0.7667\n",
      "Epoch 130/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 3.6728e-06 - acc: 1.0000 - val_loss: 1.6965 - val_acc: 0.7635\n",
      "Epoch 131/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0294 - acc: 0.9946 - val_loss: 1.7219 - val_acc: 0.7698\n",
      "Epoch 132/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.6994e-05 - acc: 1.0000 - val_loss: 1.7785 - val_acc: 0.7508\n",
      "Epoch 133/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 5.6186e-06 - acc: 1.0000 - val_loss: 1.7972 - val_acc: 0.7476\n",
      "Epoch 134/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0023 - acc: 0.9993 - val_loss: 1.8905 - val_acc: 0.7381\n",
      "Epoch 135/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 6.8285e-06 - acc: 1.0000 - val_loss: 1.8023 - val_acc: 0.7540\n",
      "Epoch 136/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0057 - acc: 0.9980 - val_loss: 1.9107 - val_acc: 0.7381\n",
      "Epoch 137/500\n",
      "1475/1475 [==============================] - 79s 54ms/step - loss: 1.4495e-05 - acc: 1.0000 - val_loss: 1.7348 - val_acc: 0.7540\n",
      "Epoch 138/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 7.4455e-06 - acc: 1.0000 - val_loss: 1.7292 - val_acc: 0.7587\n",
      "Epoch 139/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0152 - acc: 0.9953 - val_loss: 1.7407 - val_acc: 0.7635\n",
      "Epoch 140/500\n",
      "1475/1475 [==============================] - 76s 52ms/step - loss: 2.1935e-05 - acc: 1.0000 - val_loss: 1.7736 - val_acc: 0.7540\n",
      "Epoch 141/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 3.5107e-06 - acc: 1.0000 - val_loss: 1.7930 - val_acc: 0.7556\n",
      "Epoch 142/500\n",
      "1475/1475 [==============================] - 79s 54ms/step - loss: 2.3132e-06 - acc: 1.0000 - val_loss: 1.7792 - val_acc: 0.7667\n",
      "Epoch 143/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0332 - acc: 0.9932 - val_loss: 1.7702 - val_acc: 0.7635\n",
      "Epoch 144/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 8.7513e-06 - acc: 1.0000 - val_loss: 1.8045 - val_acc: 0.7476\n",
      "Epoch 145/500\n",
      "1475/1475 [==============================] - 79s 54ms/step - loss: 2.0128e-06 - acc: 1.0000 - val_loss: 1.7936 - val_acc: 0.7508\n",
      "Epoch 146/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0217 - acc: 0.9939 - val_loss: 1.8378 - val_acc: 0.7587\n",
      "Epoch 147/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.4552e-04 - acc: 1.0000 - val_loss: 1.7871 - val_acc: 0.7619\n",
      "Epoch 148/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 5.7080e-06 - acc: 1.0000 - val_loss: 1.8433 - val_acc: 0.7492\n",
      "Epoch 149/500\n",
      "1475/1475 [==============================] - 76s 52ms/step - loss: 1.3595e-06 - acc: 1.0000 - val_loss: 1.8330 - val_acc: 0.7476\n",
      "Epoch 150/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0106 - acc: 0.9973 - val_loss: 1.7930 - val_acc: 0.7651\n",
      "Epoch 151/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 5.5401e-06 - acc: 1.0000 - val_loss: 1.8857 - val_acc: 0.7460\n",
      "Epoch 152/500\n",
      "1475/1475 [==============================] - 77s 53ms/step - loss: 2.3210e-06 - acc: 1.0000 - val_loss: 1.8935 - val_acc: 0.7476\n",
      "Epoch 153/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0268 - acc: 0.9966 - val_loss: 1.7915 - val_acc: 0.7714\n",
      "Epoch 154/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.0917e-05 - acc: 1.0000 - val_loss: 1.8152 - val_acc: 0.7619\n",
      "Epoch 155/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.5470e-06 - acc: 1.0000 - val_loss: 1.8288 - val_acc: 0.7587\n",
      "Epoch 156/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 9.7296e-07 - acc: 1.0000 - val_loss: 1.8918 - val_acc: 0.7524\n",
      "Epoch 157/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0239 - acc: 0.9959 - val_loss: 1.7889 - val_acc: 0.7746\n",
      "Epoch 158/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 6.4552e-06 - acc: 1.0000 - val_loss: 1.7837 - val_acc: 0.7619\n",
      "Epoch 159/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 5.9057e-06 - acc: 1.0000 - val_loss: 1.8700 - val_acc: 0.7556\n",
      "Epoch 160/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0042 - acc: 0.9973 - val_loss: 2.0574 - val_acc: 0.7492\n",
      "Epoch 161/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1690e-05 - acc: 1.0000 - val_loss: 1.9125 - val_acc: 0.7619\n",
      "Epoch 162/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 2.3474e-06 - acc: 1.0000 - val_loss: 1.8438 - val_acc: 0.7635\n",
      "Epoch 163/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 2.2715e-06 - acc: 1.0000 - val_loss: 1.8904 - val_acc: 0.7635\n",
      "Epoch 164/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0019 - acc: 0.9993 - val_loss: 1.9291 - val_acc: 0.7571\n",
      "Epoch 165/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.4563e-06 - acc: 1.0000 - val_loss: 1.9104 - val_acc: 0.7571\n",
      "Epoch 166/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.8243e-06 - acc: 1.0000 - val_loss: 3.0467 - val_acc: 0.6889\n",
      "Epoch 167/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0136 - acc: 0.9966 - val_loss: 2.0657 - val_acc: 0.7429\n",
      "Epoch 168/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 2.3448e-06 - acc: 1.0000 - val_loss: 1.9508 - val_acc: 0.7476\n",
      "Epoch 169/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.0585e-06 - acc: 1.0000 - val_loss: 1.9848 - val_acc: 0.7476\n",
      "Epoch 170/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 0.0075 - acc: 0.9986 - val_loss: 1.9249 - val_acc: 0.7460\n",
      "Epoch 171/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.6106e-06 - acc: 1.0000 - val_loss: 1.9482 - val_acc: 0.7476\n",
      "Epoch 172/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 7.4424e-07 - acc: 1.0000 - val_loss: 1.8803 - val_acc: 0.7746\n",
      "Epoch 173/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0114 - acc: 0.9966 - val_loss: 1.9578 - val_acc: 0.7508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 9.4194e-07 - acc: 1.0000 - val_loss: 1.9508 - val_acc: 0.7524\n",
      "Epoch 175/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 5.5116e-07 - acc: 1.0000 - val_loss: 1.9639 - val_acc: 0.7524\n",
      "Epoch 176/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0172 - acc: 0.9959 - val_loss: 2.3483 - val_acc: 0.7175\n",
      "Epoch 177/500\n",
      "1475/1475 [==============================] - 79s 54ms/step - loss: 6.1278e-05 - acc: 1.0000 - val_loss: 2.1358 - val_acc: 0.7333\n",
      "Epoch 178/500\n",
      "1475/1475 [==============================] - 76s 52ms/step - loss: 2.4127e-06 - acc: 1.0000 - val_loss: 2.0291 - val_acc: 0.7429\n",
      "Epoch 179/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 9.8856e-07 - acc: 1.0000 - val_loss: 1.9624 - val_acc: 0.7508\n",
      "Epoch 180/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0127 - acc: 0.9966 - val_loss: 2.2497 - val_acc: 0.7238\n",
      "Epoch 181/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 8.2151e-06 - acc: 1.0000 - val_loss: 2.1312 - val_acc: 0.7397\n",
      "Epoch 182/500\n",
      "1475/1475 [==============================] - 77s 53ms/step - loss: 2.1330e-06 - acc: 1.0000 - val_loss: 2.0803 - val_acc: 0.7381\n",
      "Epoch 183/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 4.2475e-06 - acc: 1.0000 - val_loss: 2.0776 - val_acc: 0.7302\n",
      "Epoch 184/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 6.6582e-04 - acc: 1.0000 - val_loss: 2.0773 - val_acc: 0.7476\n",
      "Epoch 185/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.3163e-06 - acc: 1.0000 - val_loss: 2.0800 - val_acc: 0.7492\n",
      "Epoch 186/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0027 - acc: 0.9986 - val_loss: 2.3684 - val_acc: 0.7270\n",
      "Epoch 187/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 5.9159e-06 - acc: 1.0000 - val_loss: 2.0869 - val_acc: 0.7492\n",
      "Epoch 188/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.2688e-06 - acc: 1.0000 - val_loss: 2.0814 - val_acc: 0.7508\n",
      "Epoch 189/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 2.7940e-06 - acc: 1.0000 - val_loss: 2.0784 - val_acc: 0.7508\n",
      "Epoch 190/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0187 - acc: 0.9953 - val_loss: 2.4915 - val_acc: 0.7270\n",
      "Epoch 191/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 2.1284e-05 - acc: 1.0000 - val_loss: 2.3063 - val_acc: 0.7381\n",
      "Epoch 192/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.8582e-05 - acc: 1.0000 - val_loss: 2.0608 - val_acc: 0.7444\n",
      "Epoch 193/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 3.0570e-07 - acc: 1.0000 - val_loss: 2.0187 - val_acc: 0.7587\n",
      "Epoch 194/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0018 - acc: 0.9993 - val_loss: 2.1955 - val_acc: 0.7492\n",
      "Epoch 195/500\n",
      "1475/1475 [==============================] - 76s 51ms/step - loss: 1.3674e-06 - acc: 1.0000 - val_loss: 2.1348 - val_acc: 0.7492\n",
      "Epoch 196/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 2.0083e-06 - acc: 1.0000 - val_loss: 2.0316 - val_acc: 0.7524\n",
      "Epoch 197/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0056 - acc: 0.9980 - val_loss: 2.0227 - val_acc: 0.7556\n",
      "Epoch 198/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 7.7573e-07 - acc: 1.0000 - val_loss: 2.0350 - val_acc: 0.7524\n",
      "Epoch 199/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 5.9947e-07 - acc: 1.0000 - val_loss: 2.0459 - val_acc: 0.7540\n",
      "Epoch 200/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 3.9227e-07 - acc: 1.0000 - val_loss: 2.0662 - val_acc: 0.7571\n",
      "Epoch 201/500\n",
      "1475/1475 [==============================] - 76s 52ms/step - loss: 0.0074 - acc: 0.9980 - val_loss: 2.0801 - val_acc: 0.7667\n",
      "Epoch 202/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.6215e-05 - acc: 1.0000 - val_loss: 2.0280 - val_acc: 0.7746\n",
      "Epoch 203/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 3.1939e-06 - acc: 1.0000 - val_loss: 2.0115 - val_acc: 0.7635\n",
      "Epoch 204/500\n",
      "1475/1475 [==============================] - 80s 54ms/step - loss: 2.9152e-07 - acc: 1.0000 - val_loss: 2.0409 - val_acc: 0.7603\n",
      "Epoch 205/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0037 - acc: 0.9973 - val_loss: 2.5024 - val_acc: 0.7254\n",
      "Epoch 206/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 4.4668e-05 - acc: 1.0000 - val_loss: 2.0893 - val_acc: 0.7540\n",
      "Epoch 207/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 3.5618e-07 - acc: 1.0000 - val_loss: 2.0744 - val_acc: 0.7524\n",
      "Epoch 208/500\n",
      "1475/1475 [==============================] - 79s 54ms/step - loss: 1.8277e-07 - acc: 1.0000 - val_loss: 2.0546 - val_acc: 0.7524\n",
      "Epoch 209/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 0.0023 - acc: 0.9993 - val_loss: 2.0608 - val_acc: 0.7556\n",
      "Epoch 210/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 4.4124e-07 - acc: 1.0000 - val_loss: 2.1918 - val_acc: 0.7317\n",
      "Epoch 211/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 4.7575e-07 - acc: 1.0000 - val_loss: 2.0053 - val_acc: 0.7683\n",
      "Epoch 212/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 7.7752e-04 - acc: 1.0000 - val_loss: 2.0509 - val_acc: 0.7714\n",
      "Epoch 213/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 5.0323e-06 - acc: 1.0000 - val_loss: 2.0385 - val_acc: 0.7794\n",
      "Epoch 214/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 4.8146e-07 - acc: 1.0000 - val_loss: 2.0835 - val_acc: 0.7619\n",
      "Epoch 215/500\n",
      "1475/1475 [==============================] - 77s 53ms/step - loss: 3.1055e-07 - acc: 1.0000 - val_loss: 2.0491 - val_acc: 0.7762\n",
      "Epoch 216/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 3.6908e-07 - acc: 1.0000 - val_loss: 2.1457 - val_acc: 0.7556\n",
      "Epoch 217/500\n",
      "1475/1475 [==============================] - 76s 52ms/step - loss: 0.0011 - acc: 0.9993 - val_loss: 2.2708 - val_acc: 0.7460\n",
      "Epoch 218/500\n",
      "1475/1475 [==============================] - 80s 54ms/step - loss: 3.1848e-06 - acc: 1.0000 - val_loss: 2.2053 - val_acc: 0.7476\n",
      "Epoch 219/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 4.8935e-07 - acc: 1.0000 - val_loss: 2.2463 - val_acc: 0.7492\n",
      "Epoch 220/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0033 - acc: 0.9993 - val_loss: 2.2781 - val_acc: 0.7508\n",
      "Epoch 221/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 3.1463e-07 - acc: 1.0000 - val_loss: 2.1894 - val_acc: 0.7587\n",
      "Epoch 222/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 2.3317e-07 - acc: 1.0000 - val_loss: 2.1794 - val_acc: 0.7683\n",
      "Epoch 223/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0016 - acc: 0.9986 - val_loss: 2.2454 - val_acc: 0.7635\n",
      "Epoch 224/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 3.1214e-06 - acc: 1.0000 - val_loss: 2.1988 - val_acc: 0.7619\n",
      "Epoch 225/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 2.4303e-07 - acc: 1.0000 - val_loss: 2.1728 - val_acc: 0.7587\n",
      "Epoch 226/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 2.8392e-04 - acc: 1.0000 - val_loss: 2.3848 - val_acc: 0.7492\n",
      "Epoch 227/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.8435e-06 - acc: 1.0000 - val_loss: 2.2958 - val_acc: 0.7587\n",
      "Epoch 228/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 2.6556e-06 - acc: 1.0000 - val_loss: 2.2613 - val_acc: 0.7603\n",
      "Epoch 229/500\n",
      "1475/1475 [==============================] - 76s 52ms/step - loss: 3.0457e-07 - acc: 1.0000 - val_loss: 2.2279 - val_acc: 0.7603\n",
      "Epoch 230/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 6.3811e-04 - acc: 1.0000 - val_loss: 2.2764 - val_acc: 0.7476\n",
      "Epoch 231/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1475/1475 [==============================] - 79s 53ms/step - loss: 1.7610e-05 - acc: 1.0000 - val_loss: 2.1912 - val_acc: 0.7603\n",
      "Epoch 232/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 6.3335e-07 - acc: 1.0000 - val_loss: 2.2249 - val_acc: 0.7508\n",
      "Epoch 233/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 5.8665e-04 - acc: 1.0000 - val_loss: 2.1628 - val_acc: 0.7556\n",
      "Epoch 234/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 5.3433e-07 - acc: 1.0000 - val_loss: 2.1828 - val_acc: 0.7619\n",
      "Epoch 235/500\n",
      "1475/1475 [==============================] - 80s 54ms/step - loss: 2.0100e-07 - acc: 1.0000 - val_loss: 2.2917 - val_acc: 0.7540\n",
      "Epoch 236/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 0.0081 - acc: 0.9973 - val_loss: 2.2280 - val_acc: 0.7587\n",
      "Epoch 237/500\n",
      "1475/1475 [==============================] - 76s 52ms/step - loss: 2.4690e-06 - acc: 1.0000 - val_loss: 2.2286 - val_acc: 0.7587\n",
      "Epoch 238/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 6.4720e-07 - acc: 1.0000 - val_loss: 2.2284 - val_acc: 0.7540\n",
      "Epoch 239/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 7.9819e-06 - acc: 1.0000 - val_loss: 2.2420 - val_acc: 0.7603\n",
      "Epoch 240/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 3.6896e-07 - acc: 1.0000 - val_loss: 2.2911 - val_acc: 0.7571\n",
      "Epoch 241/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.8060e-05 - acc: 1.0000 - val_loss: 2.2725 - val_acc: 0.7508\n",
      "Epoch 242/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 2.3280e-07 - acc: 1.0000 - val_loss: 2.2969 - val_acc: 0.7508\n",
      "Epoch 243/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0070 - acc: 0.9993 - val_loss: 2.2490 - val_acc: 0.7524\n",
      "Epoch 244/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 1.5930e-07 - acc: 1.0000 - val_loss: 2.2521 - val_acc: 0.7524\n",
      "Epoch 245/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 2.5681e-07 - acc: 1.0000 - val_loss: 2.2224 - val_acc: 0.7603\n",
      "Epoch 246/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 1.4523e-07 - acc: 1.0000 - val_loss: 2.2466 - val_acc: 0.7571\n",
      "Epoch 247/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0012 - acc: 0.9993 - val_loss: 2.2694 - val_acc: 0.7413\n",
      "Epoch 248/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.3097e-07 - acc: 1.0000 - val_loss: 2.2677 - val_acc: 0.7413\n",
      "Epoch 249/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.7300e-07 - acc: 1.0000 - val_loss: 2.2554 - val_acc: 0.7429\n",
      "Epoch 250/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 2.1947e-07 - acc: 1.0000 - val_loss: 2.2931 - val_acc: 0.7476\n",
      "Epoch 251/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.2729e-07 - acc: 1.0000 - val_loss: 2.3210 - val_acc: 0.7492\n",
      "Epoch 252/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.3465e-07 - acc: 1.0000 - val_loss: 2.6125 - val_acc: 0.7302\n",
      "Epoch 253/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 6.0339e-05 - acc: 1.0000 - val_loss: 2.3074 - val_acc: 0.7556\n",
      "Epoch 254/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 2.6356e-07 - acc: 1.0000 - val_loss: 2.2662 - val_acc: 0.7540\n",
      "Epoch 255/500\n",
      "1475/1475 [==============================] - 76s 51ms/step - loss: 1.4895e-07 - acc: 1.0000 - val_loss: 2.3338 - val_acc: 0.7476\n",
      "Epoch 256/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 3.2208e-07 - acc: 1.0000 - val_loss: 2.5213 - val_acc: 0.7444\n",
      "Epoch 257/500\n",
      "1475/1475 [==============================] - 76s 52ms/step - loss: 6.7696e-04 - acc: 1.0000 - val_loss: 2.2837 - val_acc: 0.7540\n",
      "Epoch 258/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.9090e-05 - acc: 1.0000 - val_loss: 2.2918 - val_acc: 0.7540\n",
      "Epoch 259/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.9142e-07 - acc: 1.0000 - val_loss: 2.3753 - val_acc: 0.7603\n",
      "Epoch 260/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.4366e-07 - acc: 1.0000 - val_loss: 2.5016 - val_acc: 0.7492\n",
      "Epoch 261/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 6.5482e-05 - acc: 1.0000 - val_loss: 3.3182 - val_acc: 0.6825\n",
      "Epoch 262/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0038 - acc: 0.9993 - val_loss: 2.3168 - val_acc: 0.7524\n",
      "Epoch 263/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.3461e-07 - acc: 1.0000 - val_loss: 2.3195 - val_acc: 0.7508\n",
      "Epoch 264/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.3178e-07 - acc: 1.0000 - val_loss: 2.3268 - val_acc: 0.7524\n",
      "Epoch 265/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0034 - acc: 0.9986 - val_loss: 2.2476 - val_acc: 0.7619\n",
      "Epoch 266/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 2.2692e-06 - acc: 1.0000 - val_loss: 2.3007 - val_acc: 0.7524\n",
      "Epoch 267/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.4418e-07 - acc: 1.0000 - val_loss: 2.3372 - val_acc: 0.7492\n",
      "Epoch 268/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 3.6351e-07 - acc: 1.0000 - val_loss: 2.2997 - val_acc: 0.7635\n",
      "Epoch 269/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.2802e-07 - acc: 1.0000 - val_loss: 2.3651 - val_acc: 0.7508\n",
      "Epoch 270/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.2972e-07 - acc: 1.0000 - val_loss: 2.3708 - val_acc: 0.7492\n",
      "Epoch 271/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.2980e-07 - acc: 1.0000 - val_loss: 2.3269 - val_acc: 0.7619\n",
      "Epoch 272/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 8.9461e-06 - acc: 1.0000 - val_loss: 2.7068 - val_acc: 0.7270\n",
      "Epoch 273/500\n",
      "1475/1475 [==============================] - 80s 54ms/step - loss: 1.7704e-07 - acc: 1.0000 - val_loss: 2.5293 - val_acc: 0.7444\n",
      "Epoch 274/500\n",
      "1475/1475 [==============================] - 77s 53ms/step - loss: 1.5909e-07 - acc: 1.0000 - val_loss: 2.5882 - val_acc: 0.7444\n",
      "Epoch 275/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 4.7656e-05 - acc: 1.0000 - val_loss: 2.6023 - val_acc: 0.7381\n",
      "Epoch 276/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.4374e-07 - acc: 1.0000 - val_loss: 2.5165 - val_acc: 0.7444\n",
      "Epoch 277/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 4.6196e-06 - acc: 1.0000 - val_loss: 2.5507 - val_acc: 0.7524\n",
      "Epoch 278/500\n",
      "1475/1475 [==============================] - 77s 53ms/step - loss: 1.6689e-07 - acc: 1.0000 - val_loss: 2.4097 - val_acc: 0.7508\n",
      "Epoch 279/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.4321e-07 - acc: 1.0000 - val_loss: 2.4680 - val_acc: 0.7540\n",
      "Epoch 280/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.4523e-07 - acc: 1.0000 - val_loss: 2.4216 - val_acc: 0.7587\n",
      "Epoch 281/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0017 - acc: 0.9986 - val_loss: 3.0395 - val_acc: 0.7175\n",
      "Epoch 282/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 2.3987e-06 - acc: 1.0000 - val_loss: 2.8114 - val_acc: 0.7159\n",
      "Epoch 283/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 2.6659e-07 - acc: 1.0000 - val_loss: 2.6240 - val_acc: 0.7238\n",
      "Epoch 284/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.3812e-07 - acc: 1.0000 - val_loss: 2.4607 - val_acc: 0.7508\n",
      "Epoch 285/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.3028e-07 - acc: 1.0000 - val_loss: 2.4415 - val_acc: 0.7619\n",
      "Epoch 286/500\n",
      "1475/1475 [==============================] - 76s 52ms/step - loss: 1.1509e-04 - acc: 1.0000 - val_loss: 2.4483 - val_acc: 0.7651\n",
      "Epoch 287/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.4422e-07 - acc: 1.0000 - val_loss: 2.4557 - val_acc: 0.7587\n",
      "Epoch 288/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1475/1475 [==============================] - 78s 53ms/step - loss: 6.2200e-06 - acc: 1.0000 - val_loss: 2.7442 - val_acc: 0.7238\n",
      "Epoch 289/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 9.5104e-04 - acc: 0.9993 - val_loss: 2.8191 - val_acc: 0.7143\n",
      "Epoch 290/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.8552e-07 - acc: 1.0000 - val_loss: 2.5140 - val_acc: 0.7476\n",
      "Epoch 291/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 2.0371e-07 - acc: 1.0000 - val_loss: 2.5747 - val_acc: 0.7429\n",
      "Epoch 292/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0018 - acc: 0.9993 - val_loss: 2.9388 - val_acc: 0.7238\n",
      "Epoch 293/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 2.9571e-05 - acc: 1.0000 - val_loss: 2.5115 - val_acc: 0.7524\n",
      "Epoch 294/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.3125e-07 - acc: 1.0000 - val_loss: 2.5083 - val_acc: 0.7524\n",
      "Epoch 295/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.2163e-07 - acc: 1.0000 - val_loss: 2.5107 - val_acc: 0.7524\n",
      "Epoch 296/500\n",
      "1475/1475 [==============================] - 76s 52ms/step - loss: 0.0107 - acc: 0.9980 - val_loss: 2.6932 - val_acc: 0.7270\n",
      "Epoch 297/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 5.2397e-07 - acc: 1.0000 - val_loss: 2.6387 - val_acc: 0.7333\n",
      "Epoch 298/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.8366e-07 - acc: 1.0000 - val_loss: 2.5684 - val_acc: 0.7492\n",
      "Epoch 299/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 2.4267e-07 - acc: 1.0000 - val_loss: 2.4599 - val_acc: 0.7619\n",
      "Epoch 300/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1225e-06 - acc: 1.0000 - val_loss: 2.4151 - val_acc: 0.7619\n",
      "Epoch 301/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 2.6655e-07 - acc: 1.0000 - val_loss: 2.5083 - val_acc: 0.7556\n",
      "Epoch 302/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.3440e-07 - acc: 1.0000 - val_loss: 2.7151 - val_acc: 0.7460\n",
      "Epoch 303/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 0.0012 - acc: 0.9986 - val_loss: 2.4196 - val_acc: 0.7556\n",
      "Epoch 304/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 9.4868e-07 - acc: 1.0000 - val_loss: 2.6021 - val_acc: 0.7460\n",
      "Epoch 305/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.2151e-07 - acc: 1.0000 - val_loss: 2.5563 - val_acc: 0.7429\n",
      "Epoch 306/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.2450e-07 - acc: 1.0000 - val_loss: 2.5246 - val_acc: 0.7429\n",
      "Epoch 307/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 4.9379e-04 - acc: 1.0000 - val_loss: 2.8405 - val_acc: 0.7365\n",
      "Epoch 308/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.5029e-06 - acc: 1.0000 - val_loss: 2.3783 - val_acc: 0.7492\n",
      "Epoch 309/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 2.2101e-07 - acc: 1.0000 - val_loss: 2.4403 - val_acc: 0.7540\n",
      "Epoch 310/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.3723e-07 - acc: 1.0000 - val_loss: 2.4488 - val_acc: 0.7556\n",
      "Epoch 311/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 0.0035 - acc: 0.9993 - val_loss: 2.7837 - val_acc: 0.7302\n",
      "Epoch 312/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.5657e-05 - acc: 1.0000 - val_loss: 2.5826 - val_acc: 0.7476\n",
      "Epoch 313/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 2.4072e-07 - acc: 1.0000 - val_loss: 2.4727 - val_acc: 0.7492\n",
      "Epoch 314/500\n",
      "1475/1475 [==============================] - 76s 52ms/step - loss: 1.4535e-07 - acc: 1.0000 - val_loss: 2.3801 - val_acc: 0.7635\n",
      "Epoch 315/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.2313e-07 - acc: 1.0000 - val_loss: 2.3940 - val_acc: 0.7619\n",
      "Epoch 316/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.2159e-07 - acc: 1.0000 - val_loss: 2.5398 - val_acc: 0.7508\n",
      "Epoch 317/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.2208e-07 - acc: 1.0000 - val_loss: 2.4023 - val_acc: 0.7619\n",
      "Epoch 318/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 5.5567e-04 - acc: 1.0000 - val_loss: 2.3910 - val_acc: 0.7635\n",
      "Epoch 319/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.2907e-07 - acc: 1.0000 - val_loss: 2.3891 - val_acc: 0.7603\n",
      "Epoch 320/500\n",
      "1475/1475 [==============================] - 79s 54ms/step - loss: 1.7283e-07 - acc: 1.0000 - val_loss: 2.3926 - val_acc: 0.7571\n",
      "Epoch 321/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.2669e-07 - acc: 1.0000 - val_loss: 2.3920 - val_acc: 0.7540\n",
      "Epoch 322/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.9377e-07 - acc: 1.0000 - val_loss: 2.4725 - val_acc: 0.7508\n",
      "Epoch 323/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.2006e-07 - acc: 1.0000 - val_loss: 2.4246 - val_acc: 0.7524\n",
      "Epoch 324/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.8027e-05 - acc: 1.0000 - val_loss: 2.4152 - val_acc: 0.7524\n",
      "Epoch 325/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 8.4859e-07 - acc: 1.0000 - val_loss: 2.4132 - val_acc: 0.7540\n",
      "Epoch 326/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.2107e-07 - acc: 1.0000 - val_loss: 2.4706 - val_acc: 0.7540\n",
      "Epoch 327/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.2289e-07 - acc: 1.0000 - val_loss: 2.4667 - val_acc: 0.7571\n",
      "Epoch 328/500\n",
      "1475/1475 [==============================] - 77s 53ms/step - loss: 6.1790e-07 - acc: 1.0000 - val_loss: 2.4250 - val_acc: 0.7587\n",
      "Epoch 329/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 3.4261e-06 - acc: 1.0000 - val_loss: 2.5718 - val_acc: 0.7508\n",
      "Epoch 330/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.2503e-07 - acc: 1.0000 - val_loss: 2.4860 - val_acc: 0.7524\n",
      "Epoch 331/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 0.0017 - acc: 0.9986 - val_loss: 2.4626 - val_acc: 0.7603\n",
      "Epoch 332/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.3424e-07 - acc: 1.0000 - val_loss: 2.4853 - val_acc: 0.7571\n",
      "Epoch 333/500\n",
      "1475/1475 [==============================] - 79s 54ms/step - loss: 1.2527e-07 - acc: 1.0000 - val_loss: 2.4824 - val_acc: 0.7603\n",
      "Epoch 334/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1953e-07 - acc: 1.0000 - val_loss: 2.4926 - val_acc: 0.7587\n",
      "Epoch 335/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 1.1961e-07 - acc: 1.0000 - val_loss: 2.4996 - val_acc: 0.7587\n",
      "Epoch 336/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.2479e-07 - acc: 1.0000 - val_loss: 2.4854 - val_acc: 0.7571\n",
      "Epoch 337/500\n",
      "1475/1475 [==============================] - 76s 52ms/step - loss: 1.1937e-07 - acc: 1.0000 - val_loss: 2.4807 - val_acc: 0.7587\n",
      "Epoch 338/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 2.4818 - val_acc: 0.7619\n",
      "Epoch 339/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1978e-07 - acc: 1.0000 - val_loss: 2.5911 - val_acc: 0.7603\n",
      "Epoch 340/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.2103e-07 - acc: 1.0000 - val_loss: 2.6279 - val_acc: 0.7556\n",
      "Epoch 341/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 2.5360 - val_acc: 0.7635\n",
      "Epoch 342/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5237 - val_acc: 0.7587\n",
      "Epoch 343/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 2.2828e-07 - acc: 1.0000 - val_loss: 2.5018 - val_acc: 0.7587\n",
      "Epoch 344/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 2.3535e-05 - acc: 1.0000 - val_loss: 2.4983 - val_acc: 0.7492\n",
      "Epoch 345/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1475/1475 [==============================] - 78s 53ms/step - loss: 2.5148e-07 - acc: 1.0000 - val_loss: 3.2129 - val_acc: 0.7111\n",
      "Epoch 346/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 1.4047e-07 - acc: 1.0000 - val_loss: 2.5110 - val_acc: 0.7540\n",
      "Epoch 347/500\n",
      "1475/1475 [==============================] - 79s 54ms/step - loss: 0.0022 - acc: 0.9993 - val_loss: 2.6552 - val_acc: 0.7397\n",
      "Epoch 348/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.3016e-07 - acc: 1.0000 - val_loss: 2.6462 - val_acc: 0.7397\n",
      "Epoch 349/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.9850e-07 - acc: 1.0000 - val_loss: 2.5564 - val_acc: 0.7540\n",
      "Epoch 350/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1990e-07 - acc: 1.0000 - val_loss: 2.5573 - val_acc: 0.7540\n",
      "Epoch 351/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.2220e-07 - acc: 1.0000 - val_loss: 2.4353 - val_acc: 0.7651\n",
      "Epoch 352/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 3.8036e-06 - acc: 1.0000 - val_loss: 2.4818 - val_acc: 0.7619\n",
      "Epoch 353/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 3.8807e-06 - acc: 1.0000 - val_loss: 2.4731 - val_acc: 0.7492\n",
      "Epoch 354/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.2232e-07 - acc: 1.0000 - val_loss: 2.4846 - val_acc: 0.7508\n",
      "Epoch 355/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.2034e-07 - acc: 1.0000 - val_loss: 2.5315 - val_acc: 0.7460\n",
      "Epoch 356/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.6221e-07 - acc: 1.0000 - val_loss: 2.5035 - val_acc: 0.7476\n",
      "Epoch 357/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.2070e-07 - acc: 1.0000 - val_loss: 2.4955 - val_acc: 0.7444\n",
      "Epoch 358/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 1.2272e-07 - acc: 1.0000 - val_loss: 2.6484 - val_acc: 0.7476\n",
      "Epoch 359/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.2046e-07 - acc: 1.0000 - val_loss: 2.5279 - val_acc: 0.7571\n",
      "Epoch 360/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.2272e-07 - acc: 1.0000 - val_loss: 2.5411 - val_acc: 0.7492\n",
      "Epoch 361/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1969e-07 - acc: 1.0000 - val_loss: 2.5178 - val_acc: 0.7571\n",
      "Epoch 362/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.9829e-04 - acc: 1.0000 - val_loss: 2.5177 - val_acc: 0.7476\n",
      "Epoch 363/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.2652e-07 - acc: 1.0000 - val_loss: 2.5665 - val_acc: 0.7476\n",
      "Epoch 364/500\n",
      "1475/1475 [==============================] - 77s 53ms/step - loss: 1.1945e-07 - acc: 1.0000 - val_loss: 2.5381 - val_acc: 0.7429\n",
      "Epoch 365/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1937e-07 - acc: 1.0000 - val_loss: 2.5572 - val_acc: 0.7476\n",
      "Epoch 366/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 1.2070e-07 - acc: 1.0000 - val_loss: 2.6779 - val_acc: 0.7429\n",
      "Epoch 367/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1937e-07 - acc: 1.0000 - val_loss: 2.6004 - val_acc: 0.7444\n",
      "Epoch 368/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.4115e-07 - acc: 1.0000 - val_loss: 2.5237 - val_acc: 0.7508\n",
      "Epoch 369/500\n",
      "1475/1475 [==============================] - 76s 52ms/step - loss: 1.2070e-07 - acc: 1.0000 - val_loss: 2.4667 - val_acc: 0.7476\n",
      "Epoch 370/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1941e-07 - acc: 1.0000 - val_loss: 2.4845 - val_acc: 0.7460\n",
      "Epoch 371/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.2006e-07 - acc: 1.0000 - val_loss: 2.4853 - val_acc: 0.7492\n",
      "Epoch 372/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.2216e-07 - acc: 1.0000 - val_loss: 2.5530 - val_acc: 0.7492\n",
      "Epoch 373/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5296 - val_acc: 0.7524\n",
      "Epoch 374/500\n",
      "1475/1475 [==============================] - 79s 54ms/step - loss: 0.0035 - acc: 0.9993 - val_loss: 2.6102 - val_acc: 0.7460\n",
      "Epoch 375/500\n",
      "1475/1475 [==============================] - 79s 54ms/step - loss: 1.2119e-07 - acc: 1.0000 - val_loss: 2.6070 - val_acc: 0.7460\n",
      "Epoch 376/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1961e-07 - acc: 1.0000 - val_loss: 2.6055 - val_acc: 0.7460\n",
      "Epoch 377/500\n",
      "1475/1475 [==============================] - 80s 54ms/step - loss: 1.1961e-07 - acc: 1.0000 - val_loss: 2.5706 - val_acc: 0.7476\n",
      "Epoch 378/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5644 - val_acc: 0.7460\n",
      "Epoch 379/500\n",
      "1475/1475 [==============================] - 76s 51ms/step - loss: 0.0050 - acc: 0.9993 - val_loss: 3.2167 - val_acc: 0.7079\n",
      "Epoch 380/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.6390e-04 - acc: 1.0000 - val_loss: 2.5202 - val_acc: 0.7556\n",
      "Epoch 381/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 2.8228e-07 - acc: 1.0000 - val_loss: 2.4969 - val_acc: 0.7524\n",
      "Epoch 382/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 1.1986e-07 - acc: 1.0000 - val_loss: 2.4974 - val_acc: 0.7540\n",
      "Epoch 383/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.2382e-07 - acc: 1.0000 - val_loss: 2.5254 - val_acc: 0.7460\n",
      "Epoch 384/500\n",
      "1475/1475 [==============================] - 77s 53ms/step - loss: 1.1978e-07 - acc: 1.0000 - val_loss: 2.5737 - val_acc: 0.7460\n",
      "Epoch 385/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.3384e-07 - acc: 1.0000 - val_loss: 2.4474 - val_acc: 0.7476\n",
      "Epoch 386/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 2.4706 - val_acc: 0.7508\n",
      "Epoch 387/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1945e-07 - acc: 1.0000 - val_loss: 2.5258 - val_acc: 0.7508\n",
      "Epoch 388/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.2297e-07 - acc: 1.0000 - val_loss: 2.5076 - val_acc: 0.7492\n",
      "Epoch 389/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5076 - val_acc: 0.7492\n",
      "Epoch 390/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5076 - val_acc: 0.7492\n",
      "Epoch 391/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5076 - val_acc: 0.7492\n",
      "Epoch 392/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1925e-07 - acc: 1.0000 - val_loss: 2.4898 - val_acc: 0.7524\n",
      "Epoch 393/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1937e-07 - acc: 1.0000 - val_loss: 2.4963 - val_acc: 0.7508\n",
      "Epoch 394/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.4963 - val_acc: 0.7508\n",
      "Epoch 395/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.4963 - val_acc: 0.7508\n",
      "Epoch 396/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.4963 - val_acc: 0.7508\n",
      "Epoch 397/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.4963 - val_acc: 0.7508\n",
      "Epoch 398/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.4890 - val_acc: 0.7540\n",
      "Epoch 399/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5064 - val_acc: 0.7476\n",
      "Epoch 400/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5064 - val_acc: 0.7476\n",
      "Epoch 401/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5064 - val_acc: 0.7476\n",
      "Epoch 402/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1475/1475 [==============================] - 79s 54ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5064 - val_acc: 0.7476\n",
      "Epoch 403/500\n",
      "1475/1475 [==============================] - 79s 54ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5064 - val_acc: 0.7476\n",
      "Epoch 404/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5064 - val_acc: 0.7476\n",
      "Epoch 405/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5064 - val_acc: 0.7476\n",
      "Epoch 406/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5064 - val_acc: 0.7476\n",
      "Epoch 407/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.2030e-07 - acc: 1.0000 - val_loss: 2.4591 - val_acc: 0.7540\n",
      "Epoch 408/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.4618 - val_acc: 0.7508\n",
      "Epoch 409/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 2.4832 - val_acc: 0.7556\n",
      "Epoch 410/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1933e-07 - acc: 1.0000 - val_loss: 2.5277 - val_acc: 0.7540\n",
      "Epoch 411/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5146 - val_acc: 0.7540\n",
      "Epoch 412/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5146 - val_acc: 0.7540\n",
      "Epoch 413/500\n",
      "1475/1475 [==============================] - 77s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5428 - val_acc: 0.7524\n",
      "Epoch 414/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5428 - val_acc: 0.7524\n",
      "Epoch 415/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.2042e-07 - acc: 1.0000 - val_loss: 2.4580 - val_acc: 0.7524\n",
      "Epoch 416/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 1.2171e-07 - acc: 1.0000 - val_loss: 2.6863 - val_acc: 0.7476\n",
      "Epoch 417/500\n",
      "1475/1475 [==============================] - 79s 54ms/step - loss: 1.1937e-07 - acc: 1.0000 - val_loss: 2.4461 - val_acc: 0.7603\n",
      "Epoch 418/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.2115e-07 - acc: 1.0000 - val_loss: 2.5897 - val_acc: 0.7556\n",
      "Epoch 419/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5897 - val_acc: 0.7556\n",
      "Epoch 420/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5897 - val_acc: 0.7556\n",
      "Epoch 421/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1953e-07 - acc: 1.0000 - val_loss: 2.5917 - val_acc: 0.7556\n",
      "Epoch 422/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1982e-07 - acc: 1.0000 - val_loss: 2.5112 - val_acc: 0.7619\n",
      "Epoch 423/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5112 - val_acc: 0.7619\n",
      "Epoch 424/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1969e-07 - acc: 1.0000 - val_loss: 2.4882 - val_acc: 0.7603\n",
      "Epoch 425/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.4882 - val_acc: 0.7603\n",
      "Epoch 426/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.4882 - val_acc: 0.7603\n",
      "Epoch 427/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.4882 - val_acc: 0.7603\n",
      "Epoch 428/500\n",
      "1475/1475 [==============================] - 79s 54ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.4882 - val_acc: 0.7603\n",
      "Epoch 429/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.4885 - val_acc: 0.7619\n",
      "Epoch 430/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.2398e-07 - acc: 1.0000 - val_loss: 2.6500 - val_acc: 0.7524\n",
      "Epoch 431/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1937e-07 - acc: 1.0000 - val_loss: 2.6468 - val_acc: 0.7571\n",
      "Epoch 432/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.6468 - val_acc: 0.7571\n",
      "Epoch 433/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.6468 - val_acc: 0.7571\n",
      "Epoch 434/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 1.1969e-07 - acc: 1.0000 - val_loss: 2.5869 - val_acc: 0.7571\n",
      "Epoch 435/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 2.6235 - val_acc: 0.7524\n",
      "Epoch 436/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.6235 - val_acc: 0.7524\n",
      "Epoch 437/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.6235 - val_acc: 0.7524\n",
      "Epoch 438/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.6235 - val_acc: 0.7524\n",
      "Epoch 439/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.6235 - val_acc: 0.7524\n",
      "Epoch 440/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5956 - val_acc: 0.7540\n",
      "Epoch 441/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1969e-07 - acc: 1.0000 - val_loss: 2.5722 - val_acc: 0.7603\n",
      "Epoch 442/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5722 - val_acc: 0.7603\n",
      "Epoch 443/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1925e-07 - acc: 1.0000 - val_loss: 2.5282 - val_acc: 0.7619\n",
      "Epoch 444/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5282 - val_acc: 0.7619\n",
      "Epoch 445/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5282 - val_acc: 0.7619\n",
      "Epoch 446/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5282 - val_acc: 0.7619\n",
      "Epoch 447/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5282 - val_acc: 0.7619\n",
      "Epoch 448/500\n",
      "1475/1475 [==============================] - 76s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5420 - val_acc: 0.7603\n",
      "Epoch 449/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5420 - val_acc: 0.7603\n",
      "Epoch 450/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1925e-07 - acc: 1.0000 - val_loss: 2.5144 - val_acc: 0.7619\n",
      "Epoch 451/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5144 - val_acc: 0.7619\n",
      "Epoch 452/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5144 - val_acc: 0.7619\n",
      "Epoch 453/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5144 - val_acc: 0.7619\n",
      "Epoch 454/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5144 - val_acc: 0.7619\n",
      "Epoch 455/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5144 - val_acc: 0.7619\n",
      "Epoch 456/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5144 - val_acc: 0.7619\n",
      "Epoch 457/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 2.5561 - val_acc: 0.7603\n",
      "Epoch 458/500\n",
      "1475/1475 [==============================] - 80s 54ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5561 - val_acc: 0.7603\n",
      "Epoch 459/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5561 - val_acc: 0.7603\n",
      "Epoch 460/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5561 - val_acc: 0.7603\n",
      "Epoch 461/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5381 - val_acc: 0.7603\n",
      "Epoch 462/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5381 - val_acc: 0.7603\n",
      "Epoch 463/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1969e-07 - acc: 1.0000 - val_loss: 2.4972 - val_acc: 0.7619\n",
      "Epoch 464/500\n",
      "1475/1475 [==============================] - 76s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.4972 - val_acc: 0.7619\n",
      "Epoch 465/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.4972 - val_acc: 0.7619\n",
      "Epoch 466/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.2353e-07 - acc: 1.0000 - val_loss: 2.6164 - val_acc: 0.7524\n",
      "Epoch 467/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.6164 - val_acc: 0.7524\n",
      "Epoch 468/500\n",
      "1475/1475 [==============================] - 79s 54ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.6164 - val_acc: 0.7524\n",
      "Epoch 469/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.6164 - val_acc: 0.7524\n",
      "Epoch 470/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 2.5255 - val_acc: 0.7571\n",
      "Epoch 471/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5255 - val_acc: 0.7571\n",
      "Epoch 472/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5255 - val_acc: 0.7571\n",
      "Epoch 473/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5255 - val_acc: 0.7571\n",
      "Epoch 474/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1937e-07 - acc: 1.0000 - val_loss: 2.5914 - val_acc: 0.7556\n",
      "Epoch 475/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 2.5720 - val_acc: 0.7603\n",
      "Epoch 476/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5720 - val_acc: 0.7603\n",
      "Epoch 477/500\n",
      "1475/1475 [==============================] - 80s 54ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5720 - val_acc: 0.7603\n",
      "Epoch 478/500\n",
      "1475/1475 [==============================] - 79s 54ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5720 - val_acc: 0.7603\n",
      "Epoch 479/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5720 - val_acc: 0.7603\n",
      "Epoch 480/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1925e-07 - acc: 1.0000 - val_loss: 2.5312 - val_acc: 0.7587\n",
      "Epoch 481/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5312 - val_acc: 0.7587\n",
      "Epoch 482/500\n",
      "1475/1475 [==============================] - 79s 54ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5312 - val_acc: 0.7587\n",
      "Epoch 483/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5312 - val_acc: 0.7587\n",
      "Epoch 484/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5312 - val_acc: 0.7587\n",
      "Epoch 485/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5312 - val_acc: 0.7587\n",
      "Epoch 486/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5312 - val_acc: 0.7587\n",
      "Epoch 487/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5312 - val_acc: 0.7587\n",
      "Epoch 488/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5312 - val_acc: 0.7587\n",
      "Epoch 489/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1925e-07 - acc: 1.0000 - val_loss: 2.5157 - val_acc: 0.7603\n",
      "Epoch 490/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5157 - val_acc: 0.7603\n",
      "Epoch 491/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5157 - val_acc: 0.7603\n",
      "Epoch 492/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5157 - val_acc: 0.7603\n",
      "Epoch 493/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5433 - val_acc: 0.7587\n",
      "Epoch 494/500\n",
      "1475/1475 [==============================] - 76s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.5625 - val_acc: 0.7619\n",
      "Epoch 495/500\n",
      "1475/1475 [==============================] - 78s 53ms/step - loss: 1.1925e-07 - acc: 1.0000 - val_loss: 2.5924 - val_acc: 0.7571\n",
      "Epoch 496/500\n",
      "1475/1475 [==============================] - 80s 54ms/step - loss: 1.1961e-07 - acc: 1.0000 - val_loss: 2.4780 - val_acc: 0.7635\n",
      "Epoch 497/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 2.4952 - val_acc: 0.7619\n",
      "Epoch 498/500\n",
      "1475/1475 [==============================] - 77s 52ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.4952 - val_acc: 0.7619\n",
      "Epoch 499/500\n",
      "1475/1475 [==============================] - 77s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.4952 - val_acc: 0.7619\n",
      "Epoch 500/500\n",
      "1475/1475 [==============================] - 79s 53ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.4952 - val_acc: 0.7619\n"
     ]
    }
   ],
   "source": [
    "model.fit(np.array(x_new_train), dummy_y,\n",
    "              batch_size=30,\n",
    "              epochs=500,\n",
    "              verbose=1, \n",
    "             validation_data=(np.array(x_test[:1000]), dummy2_y[:1000]))\n",
    "model.save_weights('PvNP_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('PvNP_weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "r = 20\n",
    "print(model.predict_classes(np.expand_dims(np.array(x_test[r]), axis=0)))\n",
    "print(dummy2_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 288, 432, 8)       296       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 288, 432, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 286, 430, 8)       584       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 286, 430, 8)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 143, 215, 8)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 143, 215, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 143, 215, 16)      1168      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 143, 215, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 141, 213, 16)      2320      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 141, 213, 16)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 70, 106, 16)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 118720)            0         \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 118720)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 237442    \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 241,810\n",
      "Trainable params: 241,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(8, (3, 3), padding='same',\n",
    "                 input_shape=(288, 432, 4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(8, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(16, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(10))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.load_weights('PvNP_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "#flatten_5 might need to be changed depending on the model\n",
    "model2 = Model(inputs=model.input, outputs=model.get_layer('flatten_3').output)\n",
    "svm_x_train = []\n",
    "svm_y_train = []\n",
    "for i in range(len(x_new_train)):\n",
    "    x_1 = np.expand_dims(x_new_train[i], axis=0)\n",
    "    flatten_2_features = model2.predict(x_1)\n",
    "    svm_x_train.append(flatten_2_features)\n",
    "    svm_y_train.append(dummy_y[i])#[0] if testing out tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svm_x_train = np.array(svm_x_train)\n",
    "clf = svm.SVC(kernel='rbf', class_weight='balanced')\n",
    "dataset_size = len(svm_x_train)\n",
    "svm_x_train = np.array(svm_x_train).reshape(dataset_size,-1)\n",
    "svm_y_train = np.array(svm_y_train)\n",
    "svm_y_train = [np.where(r==1)[0][0] for r in svm_y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bryanmckinney/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(svm_x_train, svm_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_x_test = []\n",
    "svm_y_test = []\n",
    "for i in range(len(x_test)):\n",
    "    x_1 = np.expand_dims(x_test[i], axis=0)\n",
    "    #x_1 = preprocess_input(x_1)\n",
    "    flatten_2_features = model2.predict(x_1)\n",
    "    svm_x_test.append(flatten_2_features)\n",
    "    svm_y_test.append(dummy2_y[i])\n",
    "svm_x_test = np.array(svm_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1]\n",
      "[1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1\n",
      " 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1\n",
      " 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1\n",
      " 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0\n",
      " 0 0 0 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1\n",
      " 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 0\n",
      " 0 1 1 1 0 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 0\n",
      " 0 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1\n",
      " 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0\n",
      " 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 1 1 1\n",
      " 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1\n",
      " 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0 1 1 1\n",
      " 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1\n",
      " 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0\n",
      " 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 0\n",
      " 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 1 0 0\n",
      " 0 1 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0\n",
      " 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7603174603174603"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size = len(svm_x_test)\n",
    "svm_x_test = np.array(svm_x_test).reshape(dataset_size,-1)\n",
    "svm_y_test = [np.where(r==1)[0][0] for r in svm_y_test]\n",
    "print(svm_y_test)\n",
    "print(clf.predict(svm_x_test))\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(svm_y_test, clf.predict(svm_x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "#y = np.asarray(y_train)\n",
    "#x = np.asarray(x_train)\n",
    "svm_x_train = np.array(svm_x_train)\n",
    "dataset_size = len(svm_x_train)\n",
    "svm_x_train = np.array(svm_x_train).reshape(dataset_size,-1)\n",
    "svm_y_train = np.array(svm_y_train)\n",
    "#print(svm_y_train)\n",
    "#X_Train, X_Test, Y_Train, Y_Test = train_test_split(x, y)\n",
    "clf = TPOTClassifier(verbosity=2, n_jobs=-1, generations=5, config_dict = 'TPOT light')\n",
    "clf.fit(svm_x_train, svm_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf._fitted_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_x_train = np.array(svm_x_train)\n",
    "dataset_size = len(svm_x_train)\n",
    "svm_x_train = np.array(svm_x_train).reshape(dataset_size,-1)\n",
    "svm_y_train = np.array(svm_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dask.distributed import Client\n",
    "#client = Client(n_workers=4, threads_per_worker = 1)\n",
    "\n",
    "#client \n",
    "clf = TPOTClassifier(verbosity=0, n_jobs=1, generations=5, cv=2,population_size=5, config_dict = 'TPOT light')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.fit(svm_x_train, svm_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(svm_x_test)\n",
    "svm_x_test = np.array(svm_x_test).reshape(dataset_size,-1)\n",
    "svm_y_test = np.array(svm_y_test)\n",
    "clf.score(svm_x_test,svm_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2).fit(svm_x_train)\n",
    "pca_2d = pca.transform(svm_x_train)\n",
    "\n",
    "import pylab as pl\n",
    "for i in range(0, pca_2d.shape[0]):\n",
    "    if svm_y_train[i] == 0:\n",
    "        c1 = pl.scatter(pca_2d[i,0],pca_2d[i,1],c='r',    marker='+')\n",
    "    elif svm_y_train[i] == 1:\n",
    "        c2 = pl.scatter(pca_2d[i,0],pca_2d[i,1],c='g',    marker='+')\n",
    "    \n",
    "pl.legend([c1, c2], ['Non', 'Park'])\n",
    "pl.title('dataset with 2 classes and known outcomes')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118720\n"
     ]
    }
   ],
   "source": [
    "print(len(svm_x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1475"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_2d.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 28.232468, -83.51162 ], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_2d[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
